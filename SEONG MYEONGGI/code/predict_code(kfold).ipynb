{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ðŸ“„ Document type classification baseline code**\n",
    "> ë¬¸ì„œ íƒ€ìž… ë¶„ë¥˜ ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  8 11:53:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:4B:00.0 Off |                  N/A |\n",
      "| 39%   31C    P8              24W / 350W |   8182MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW, RMSprop, SGD, Adam, Adamax, Adadelta, Adagrad\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ í´ëž˜ìŠ¤\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, int(target)  # targetì„ ì •ìˆ˜ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµì„ ìœ„í•œ í•¨ìˆ˜\n",
    "def train_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"loss\": train_loss,\n",
    "        \"acc\": train_acc,\n",
    "        \"f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "# ê²€ì¦ì„ ìœ„í•œ í•¨ìˆ˜\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "            targets_list.extend(targets.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"loss\": val_loss,\n",
    "        \"acc\": val_acc,\n",
    "        \"f1\": val_f1,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'efficientnet_b5' # efficientnet_b2 or efficientnet_b5\n",
    "img_size = 224\n",
    "LR = 1e-4\n",
    "EPOCHS = 10 # 3~10\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 3140\n"
     ]
    }
   ],
   "source": [
    "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "trn_dataset = ImageDataset(\n",
    "    \"/home/train_csv/train1200(crop, flip, blur, noise).csv\",\n",
    "    # \"/home/data/augmented(flip,blur,noise)\",\n",
    "    \"/home/data/augmented1200(crop,flip,blur,noise)\",\n",
    "    transform=trn_transform\n",
    ")\n",
    "tst_dataset = ImageDataset(\n",
    "    \"/home/data/sample_submission.csv\",\n",
    "    \"/home/data/test_transform_real_denoise\",\n",
    "    transform=tst_transform\n",
    ")\n",
    "\n",
    "print(len(trn_dataset), len(tst_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', target_weights=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.target_weights = target_weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Focal Loss ê³„ì‚°\n",
    "        focal_weights = torch.ones_like(pt)\n",
    "        if self.target_weights is not None:\n",
    "            for cls, weight in self.target_weights.items():\n",
    "                focal_weights[targets == cls] *= weight\n",
    "\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss * focal_weights\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì í™” í•¨ìˆ˜ ì •ì˜\n",
    "def get_optimizer(model, name='adam', lr=1e-3):\n",
    "    if name == 'adam':\n",
    "        return Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    elif name == 'adamw':\n",
    "        return AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    elif name == 'sgd':\n",
    "        return SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-5)\n",
    "    elif name == 'rmsprop':\n",
    "        return RMSprop(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {name}\")\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ì •ì˜\n",
    "def get_loss(name='focal'):\n",
    "    if name == 'ce':\n",
    "        return nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    elif name == 'focal':\n",
    "        target_weights = {3: 15, 7: 10, 14: 10}\n",
    "        return FocalLoss(alpha=1, gamma=2, target_weights=target_weights)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss: {name}\")\n",
    "\n",
    "# ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì˜\n",
    "def get_scheduler(optimizer, name='cosine', T_0=10):\n",
    "    if name == 'cosine':\n",
    "        return CosineAnnealingWarmRestarts(optimizer, T_0=T_0)\n",
    "    elif name == 'step':\n",
    "        return StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, loss_fn, device, num_epochs):\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_ret = train_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_ret = validate(val_loader, model, loss_fn, device)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_ret['loss']:.4f}, Acc: {train_ret['acc']:.4f}, F1: {train_ret['f1']:.4f}\")\n",
    "        print(f\"Val Loss: {val_ret['loss']:.4f}, Acc: {val_ret['acc']:.4f}, F1: {val_ret['f1']:.4f}\")\n",
    "        \n",
    "        if val_ret['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_ret['f1']\n",
    "            torch.save(model.state_dict(), f'effinetb2_best_fold{fold+1}.pth')\n",
    "    \n",
    "    return best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b5.sw_in12k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b5.sw_in12k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Loss: 0.1545: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.3316, Acc: 0.7847, F1: 0.8020\n",
      "Val Loss: 0.3521, Acc: 0.9308, F1: 0.9363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2376: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 0.2496, Acc: 0.9491, F1: 0.9533\n",
      "Val Loss: 0.2123, Acc: 0.9623, F1: 0.9657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0010: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 0.1733, Acc: 0.9616, F1: 0.9653\n",
      "Val Loss: 0.1104, Acc: 0.9824, F1: 0.9833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0692: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 0.0878, Acc: 0.9796, F1: 0.9813\n",
      "Val Loss: 0.4603, Acc: 0.9626, F1: 0.9619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0011: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Train Loss: 0.0797, Acc: 0.9829, F1: 0.9842\n",
      "Val Loss: 0.0899, Acc: 0.9850, F1: 0.9844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0253: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Train Loss: 0.0336, Acc: 0.9908, F1: 0.9916\n",
      "Val Loss: 0.0503, Acc: 0.9868, F1: 0.9885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1719: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Train Loss: 0.0204, Acc: 0.9948, F1: 0.9952\n",
      "Val Loss: 0.0264, Acc: 0.9962, F1: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0011: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:09<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Train Loss: 0.0050, Acc: 0.9988, F1: 0.9989\n",
      "Val Loss: 0.0150, Acc: 0.9978, F1: 0.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Train Loss: 0.0011, Acc: 0.9995, F1: 0.9995\n",
      "Val Loss: 0.0149, Acc: 0.9983, F1: 0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Train Loss: 0.0006, Acc: 0.9998, F1: 0.9998\n",
      "Val Loss: 0.0146, Acc: 0.9984, F1: 0.9986\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b5.sw_in12k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b5.sw_in12k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Loss: 0.0536: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.2802, Acc: 0.7949, F1: 0.8110\n",
      "Val Loss: 0.2425, Acc: 0.9476, F1: 0.9536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0480: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 0.2505, Acc: 0.9478, F1: 0.9520\n",
      "Val Loss: 0.3083, Acc: 0.9454, F1: 0.9516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0645: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 0.1465, Acc: 0.9683, F1: 0.9704\n",
      "Val Loss: 0.1640, Acc: 0.9653, F1: 0.9704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0206: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 0.1094, Acc: 0.9772, F1: 0.9789\n",
      "Val Loss: 0.1118, Acc: 0.9846, F1: 0.9866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1008/1200 [02:39<00:30,  6.31it/s]"
     ]
    }
   ],
   "source": [
    "n_splits = 5 # 5Fold\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# k-fold êµì°¨ ê²€ì¦\n",
    "fold_scores = []\n",
    "fold_predictions = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(trn_dataset)):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        trn_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        sampler=train_subsampler, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        trn_dataset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=val_subsampler, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    model = timm.create_model(\n",
    "        model_name, \n",
    "        pretrained=True, \n",
    "        num_classes=17\n",
    "    ).to(device)\n",
    "    optimizer = get_optimizer(model, name='adam', lr=LR)\n",
    "    loss_fn = get_loss(name='focal')\n",
    "    scheduler = get_scheduler(optimizer, name='cosine', T_0=10)\n",
    "    \n",
    "    best_val_f1 = train_model(model, train_loader, val_loader, optimizer, scheduler, loss_fn, device, EPOCHS)\n",
    "    fold_scores.append(best_val_f1)\n",
    "\n",
    "print(f\"K-Fold CV score: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b2.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b2.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Loss: 0.6307:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 527/1500 [00:35<01:05, 14.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0391: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [01:41<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 0.7112, Acc: 0.8165, F1: 0.8282\n",
      "Val Loss: 7.9210, Acc: 0.0643, F1: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1346: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [01:41<00:00, 14.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "Train Loss: 0.1076, Acc: 0.9627, F1: 0.9656\n",
      "Val Loss: 9.1173, Acc: 0.0631, F1: 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0173: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [01:41<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "Train Loss: 0.0463, Acc: 0.9833, F1: 0.9846\n",
      "Val Loss: 9.4621, Acc: 0.0672, F1: 0.0074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.007407797496796392"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_loader = DataLoader(\n",
    "    trn_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    tst_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "final_model = timm.create_model(\n",
    "    model_name, \n",
    "    pretrained=True, \n",
    "    num_classes=17\n",
    ").to(device)\n",
    "final_optimizer = get_optimizer(final_model, name='adam', lr=LR)\n",
    "final_loss_fn = get_loss(name='focal')\n",
    "final_scheduler = get_scheduler(final_optimizer, name='cosine', T_0=10)\n",
    "\n",
    "train_model(final_model, trn_loader, val_loader, final_optimizer, final_scheduler, final_loss_fn, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:02<00:00, 35.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# ì¶”ë¡ \n",
    "final_model.load_state_dict(torch.load('/home/code/effinetb2_best_fold5.pth'))\n",
    "final_model.eval()\n",
    "\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(val_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = final_model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì €ìž¥\n",
    "pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"/home/data/pred/kfold_pred/effinetb2_best_fold5_testDenoising.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.7730\n"
     ]
    }
   ],
   "source": [
    "# ì„±ëŠ¥ í‰ê°€\n",
    "answer_df = pd.read_csv(\"/home/data/answer.csv\")\n",
    "assert (answer_df['ID'] == pred_df['ID']).all()\n",
    "\n",
    "f1 = f1_score(answer_df['target'], pred_df['target'], average='macro')\n",
    "print(f\"Macro F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
