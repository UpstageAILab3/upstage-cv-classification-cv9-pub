{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üìÑ Document type classification baseline code**\n",
    "> Î¨∏ÏÑú ÌÉÄÏûÖ Î∂ÑÎ•ò ÎåÄÌöåÏóê Ïò§Ïã† Ïó¨Îü¨Î∂Ñ ÌôòÏòÅÌï©ÎãàÎã§! üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  8 11:53:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  | 00000000:4B:00.0 Off |                  N/A |\n",
      "| 39%   31C    P8              24W / 350W |   8182MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW, RMSprop, SGD, Adam, Adamax, Adadelta, Adagrad\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏãúÎìúÎ•º Í≥†Ï†ïÌï©ÎãàÎã§.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, int(target)  # targetÏùÑ Ï†ïÏàòÎ°ú Î≥ÄÌôò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïôÏäµÏùÑ ÏúÑÌïú Ìï®Ïàò\n",
    "def train_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"loss\": train_loss,\n",
    "        \"acc\": train_acc,\n",
    "        \"f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "# Í≤ÄÏ¶ùÏùÑ ÏúÑÌïú Ìï®Ïàò\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "            targets_list.extend(targets.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"loss\": val_loss,\n",
    "        \"acc\": val_acc,\n",
    "        \"f1\": val_f1,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'efficientnet_b5' # efficientnet_b2 or efficientnet_b5\n",
    "img_size = 224\n",
    "LR = 1e-4\n",
    "EPOCHS = 10 # 3~10\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 3140\n"
     ]
    }
   ],
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
    "trn_dataset = ImageDataset(\n",
    "    \"/home/train_csv/train1200(crop, flip, blur, noise).csv\",\n",
    "    # \"/home/data/augmented(flip,blur,noise)\",\n",
    "    \"/home/data/augmented1200(crop,flip,blur,noise)\",\n",
    "    transform=trn_transform\n",
    ")\n",
    "tst_dataset = ImageDataset(\n",
    "    \"/home/data/sample_submission.csv\",\n",
    "    \"/home/data/test_transform_real_denoise\",\n",
    "    transform=tst_transform\n",
    ")\n",
    "\n",
    "print(len(trn_dataset), len(tst_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', target_weights=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.target_weights = target_weights\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Focal Loss Í≥ÑÏÇ∞\n",
    "        focal_weights = torch.ones_like(pt)\n",
    "        if self.target_weights is not None:\n",
    "            for cls, weight in self.target_weights.items():\n",
    "                focal_weights[targets == cls] *= weight\n",
    "\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss * focal_weights\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏµúÏ†ÅÌôî Ìï®Ïàò Ï†ïÏùò\n",
    "def get_optimizer(model, name='adam', lr=1e-3):\n",
    "    if name == 'adam':\n",
    "        return Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    elif name == 'adamw':\n",
    "        return AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    elif name == 'sgd':\n",
    "        return SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-5)\n",
    "    elif name == 'rmsprop':\n",
    "        return RMSprop(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {name}\")\n",
    "\n",
    "# ÏÜêÏã§ Ìï®Ïàò Ï†ïÏùò\n",
    "def get_loss(name='focal'):\n",
    "    if name == 'ce':\n",
    "        return nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    elif name == 'focal':\n",
    "        target_weights = {3: 15, 7: 10, 14: 10}\n",
    "        return FocalLoss(alpha=1, gamma=2, target_weights=target_weights)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss: {name}\")\n",
    "\n",
    "# Ïä§ÏºÄÏ§ÑÎü¨ Ï†ïÏùò\n",
    "def get_scheduler(optimizer, name='cosine', T_0=10):\n",
    "    if name == 'cosine':\n",
    "        return CosineAnnealingWarmRestarts(optimizer, T_0=T_0)\n",
    "    elif name == 'step':\n",
    "        return StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Î™®Îç∏ ÌïôÏäµ Ìï®Ïàò\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, loss_fn, device, num_epochs):\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_ret = train_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        val_ret = validate(val_loader, model, loss_fn, device)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_ret['loss']:.4f}, Acc: {train_ret['acc']:.4f}, F1: {train_ret['f1']:.4f}\")\n",
    "        print(f\"Val Loss: {val_ret['loss']:.4f}, Acc: {val_ret['acc']:.4f}, F1: {val_ret['f1']:.4f}\")\n",
    "        \n",
    "        if val_ret['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_ret['f1']\n",
    "            torch.save(model.state_dict(), f'effinetb2_best_fold{fold+1}.pth')\n",
    "    \n",
    "    return best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b5.sw_in12k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b5.sw_in12k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Loss: 0.1545: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.3316, Acc: 0.7847, F1: 0.8020\n",
      "Val Loss: 0.3521, Acc: 0.9308, F1: 0.9363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2376: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 0.2496, Acc: 0.9491, F1: 0.9533\n",
      "Val Loss: 0.2123, Acc: 0.9623, F1: 0.9657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0010: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 0.1733, Acc: 0.9616, F1: 0.9653\n",
      "Val Loss: 0.1104, Acc: 0.9824, F1: 0.9833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0692: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 0.0878, Acc: 0.9796, F1: 0.9813\n",
      "Val Loss: 0.4603, Acc: 0.9626, F1: 0.9619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0011: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Train Loss: 0.0797, Acc: 0.9829, F1: 0.9842\n",
      "Val Loss: 0.0899, Acc: 0.9850, F1: 0.9844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0253: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Train Loss: 0.0336, Acc: 0.9908, F1: 0.9916\n",
      "Val Loss: 0.0503, Acc: 0.9868, F1: 0.9885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1719: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Train Loss: 0.0204, Acc: 0.9948, F1: 0.9952\n",
      "Val Loss: 0.0264, Acc: 0.9962, F1: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0011: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:09<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Train Loss: 0.0050, Acc: 0.9988, F1: 0.9989\n",
      "Val Loss: 0.0150, Acc: 0.9978, F1: 0.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Train Loss: 0.0011, Acc: 0.9995, F1: 0.9995\n",
      "Val Loss: 0.0149, Acc: 0.9983, F1: 0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Train Loss: 0.0006, Acc: 0.9998, F1: 0.9998\n",
      "Val Loss: 0.0146, Acc: 0.9984, F1: 0.9986\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b5.sw_in12k_ft_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b5.sw_in12k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Loss: 0.0536: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.2802, Acc: 0.7949, F1: 0.8110\n",
      "Val Loss: 0.2425, Acc: 0.9476, F1: 0.9536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0480: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 0.2505, Acc: 0.9478, F1: 0.9520\n",
      "Val Loss: 0.3083, Acc: 0.9454, F1: 0.9516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0645: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 0.1465, Acc: 0.9683, F1: 0.9704\n",
      "Val Loss: 0.1640, Acc: 0.9653, F1: 0.9704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0206: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1200/1200 [03:10<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 0.1094, Acc: 0.9772, F1: 0.9789\n",
      "Val Loss: 0.1118, Acc: 0.9846, F1: 0.9866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 1008/1200 [02:39<00:30,  6.31it/s]"
     ]
    }
   ],
   "source": [
    "n_splits = 5 # 5Fold\n",
    "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# k-fold ÍµêÏ∞® Í≤ÄÏ¶ù\n",
    "fold_scores = []\n",
    "fold_predictions = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(trn_dataset)):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        trn_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        sampler=train_subsampler, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        trn_dataset, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=val_subsampler, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    model = timm.create_model(\n",
    "        model_name, \n",
    "        pretrained=True, \n",
    "        num_classes=17\n",
    "    ).to(device)\n",
    "    optimizer = get_optimizer(model, name='adam', lr=LR)\n",
    "    loss_fn = get_loss(name='focal')\n",
    "    scheduler = get_scheduler(optimizer, name='cosine', T_0=10)\n",
    "    \n",
    "    best_val_f1 = train_model(model, train_loader, val_loader, optimizer, scheduler, loss_fn, device, EPOCHS)\n",
    "    fold_scores.append(best_val_f1)\n",
    "\n",
    "print(f\"K-Fold CV score: {np.mean(fold_scores):.4f} (+/- {np.std(fold_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b2.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b2.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Loss: 0.6307:  35%|‚ñà‚ñà‚ñà‚ñå      | 527/1500 [00:35<01:05, 14.88it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0391: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [01:41<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Train Loss: 0.7112, Acc: 0.8165, F1: 0.8282\n",
      "Val Loss: 7.9210, Acc: 0.0643, F1: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1346: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [01:41<00:00, 14.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3\n",
      "Train Loss: 0.1076, Acc: 0.9627, F1: 0.9656\n",
      "Val Loss: 9.1173, Acc: 0.0631, F1: 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0173: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [01:41<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "Train Loss: 0.0463, Acc: 0.9833, F1: 0.9846\n",
      "Val Loss: 9.4621, Acc: 0.0672, F1: 0.0074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.007407797496796392"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_loader = DataLoader(\n",
    "    trn_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    tst_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "final_model = timm.create_model(\n",
    "    model_name, \n",
    "    pretrained=True, \n",
    "    num_classes=17\n",
    ").to(device)\n",
    "final_optimizer = get_optimizer(final_model, name='adam', lr=LR)\n",
    "final_loss_fn = get_loss(name='focal')\n",
    "final_scheduler = get_scheduler(final_optimizer, name='cosine', T_0=10)\n",
    "\n",
    "train_model(final_model, trn_loader, val_loader, final_optimizer, final_scheduler, final_loss_fn, device, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:02<00:00, 35.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ï∂îÎ°†\n",
    "final_model.load_state_dict(torch.load('/home/code/effinetb2_best_fold5.pth'))\n",
    "final_model.eval()\n",
    "\n",
    "preds_list = []\n",
    "\n",
    "for image, _ in tqdm(val_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = final_model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Í≤∞Í≥º Ï†ÄÏû•\n",
    "pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list\n",
    "pred_df.to_csv(\"/home/data/pred/kfold_pred/effinetb2_best_fold5_testDenoising.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.7730\n"
     ]
    }
   ],
   "source": [
    "# ÏÑ±Îä• ÌèâÍ∞Ä\n",
    "answer_df = pd.read_csv(\"/home/data/answer.csv\")\n",
    "assert (answer_df['ID'] == pred_df['ID']).all()\n",
    "\n",
    "f1 = f1_score(answer_df['target'], pred_df['target'], average='macro')\n",
    "print(f\"Macro F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
