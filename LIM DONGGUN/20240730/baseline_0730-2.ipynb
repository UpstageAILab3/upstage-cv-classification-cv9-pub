{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"OliaDaX_lwou"},"source":["# **📄 Document type classification baseline code**\n","> 문서 타입 분류 대회에 오신 여러분 환영합니다! 🎉     \n","> 아래 baseline에서는 ResNet 모델을 로드하여, 모델을 학습 및 예측 파일 생성하는 프로세스에 대해 알아보겠습니다.\n","\n","## Contents\n","- Prepare Environments\n","- Import Library & Define Functions\n","- Hyper-parameters\n","- Load Data\n","- Train Model\n","- Inference & Save File\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zkH9T_86lDSS"},"source":["## 1. Prepare Environments\n","\n","* 데이터 로드를 위한 구글 드라이브를 마운트합니다.\n","* 필요한 라이브러리를 설치합니다."]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["100480\n"]}],"source":["print(1570 * 8 * 8)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21945,"status":"ok","timestamp":1700314517484,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"pUjnEto4gIZm","outputId":"0999f10c-e1ff-428c-995b-481eec8a0b58"},"outputs":[],"source":["# 구글 드라이브 마운트, Colab을 이용하지 않는다면 패스해도 됩니다.\n","#from google.colab import drive\n","#rive.mount('/gdrive', force_remount=True)\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7640,"status":"ok","timestamp":1700314537985,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"5lFQ-gpjnN_m"},"outputs":[],"source":["# 구글 드라이브에 업로드된 대회 데이터를 압축 해제하고 로컬에 저장합니다.\n","#!tar -xvf drive/MyDrive/datasets_fin.tar > /dev/null"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# !pip install -r requirements.txt 이미 서버에 설치되어 있음"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8489,"status":"ok","timestamp":1700314558888,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"NC8V-D393wY4","outputId":"e9927325-26c4-4b89-9c51-c1d6541388d6"},"outputs":[],"source":["# 필요한 라이브러리를 설치합니다.\n","# !pip install timm 이미 서버에 설치되어 있음"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PXa_FPM73R9f"},"source":["## 2. Import Library & Define Functions\n","* 학습 및 추론에 필요한 라이브러리를 로드합니다.\n","* 학습 및 추론에 필요한 함수와 클래스를 정의합니다."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":9396,"status":"ok","timestamp":1700314592802,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"3BaoIkv5Xwa0"},"outputs":[],"source":["import os\n","import time\n","import random\n","\n","import timm\n","import torch\n","import albumentations as A\n","import pandas as pd\n","import numpy as np\n","import torch.nn as nn\n","from albumentations.pytorch import ToTensorV2\n","from torch.optim import Adam\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","### 추가내용\n","from datetime import datetime       # 폴더이름에 날짜시간을 넣기 위한 라이브러리\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# 시드를 고정합니다.\n","SEED = 42  # 재현 가능한 결과를 위해 사용할 시드 값 설정\n","\n","# 파이썬의 해시 시드를 고정하여 파이썬의 해시 값이 일정하도록 설정\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","\n","# 파이썬의 랜덤 시드를 고정하여 랜덤 연산이 재현 가능하도록 설정\n","random.seed(SEED)\n","\n","# Numpy의 랜덤 시드를 고정하여 Numpy 연산이 재현 가능하도록 설정\n","np.random.seed(SEED)\n","\n","# PyTorch의 랜덤 시드를 고정하여 PyTorch 연산이 재현 가능하도록 설정\n","torch.manual_seed(SEED)\n","\n","# CUDA 사용 시 각 디바이스의 랜덤 시드를 고정하여 CUDA 연산이 재현 가능하도록 설정\n","torch.cuda.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)  # 여러 GPU를 사용하는 경우 모두에 시드 설정\n","\n","# cuDNN에서 최적의 성능을 내기 위한 벤치마크 모드 활성화\n","torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1700314772722,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"Hyl8oAy6TZAu"},"outputs":[],"source":["# 데이터셋 클래스를 정의합니다.\n","class ImageDataset(Dataset):\n","    def __init__(self, csv, path, transform=None):\n","        \"\"\"\n","        ImageDataset 클래스의 초기화 메서드입니다.\n","        \n","        :param csv: 이미지 파일 이름과 레이블이 포함된 CSV 파일 경로\n","        :param path: 이미지 파일이 저장된 디렉토리 경로\n","        :param transform: 이미지 변환을 위한 Albumentations 변환 함수\n","        \"\"\"\n","        # CSV 파일을 읽어 데이터프레임으로 저장\n","        self.df = pd.read_csv(csv).values\n","        self.path = path  # 이미지 파일이 저장된 경로\n","        self.transform = transform  # 이미지 변환 함수\n","\n","    def __len__(self):\n","        \"\"\"\n","        데이터셋의 길이를 반환하는 메서드입니다.\n","        \n","        :return: 데이터셋의 길이\n","        \"\"\"\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        인덱스에 해당하는 데이터를 반환하는 메서드입니다.\n","        \n","        :param idx: 데이터 인덱스\n","        :return: 변환된 이미지와 해당 레이블\n","        \"\"\"\n","        name, target = self.df[idx]  # 인덱스에 해당하는 이미지 파일 이름과 레이블 가져오기\n","        img = np.array(Image.open(os.path.join(self.path, name)))  # 이미지 파일 열기\n","        if self.transform:\n","            # 이미지에 변환 적용\n","            img = self.transform(image=img)['image']\n","        return img, target  # 변환된 이미지와 레이블 반환\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":255,"status":"ok","timestamp":1700315066028,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"kTECBJfVTbdl"},"outputs":[],"source":["# 한 epoch 동안 학습을 수행하는 함수\n","def train_one_epoch(loader, model, optimizer, loss_fn, device):\n","    model.train()  # 모델을 학습 모드로 전환\n","    train_loss = 0  # 전체 손실 초기화\n","    preds_list = []  # 예측값을 저장할 리스트 초기화\n","    targets_list = []  # 실제 값을 저장할 리스트 초기화\n","\n","    # 데이터 로더를 tqdm을 이용해 진행률 표시줄과 함께 사용\n","    pbar = tqdm(loader)\n","    for image, targets in pbar:\n","        image = image.to(device)  # 이미지를 장치(GPU/CPU)로 이동\n","        targets = targets.to(device)  # 타겟을 장치(GPU/CPU)로 이동\n","\n","        model.zero_grad(set_to_none=True)  # 모델의 기울기를 초기화\n","\n","        preds = model(image)  # 모델을 통해 예측값 생성\n","        loss = loss_fn(preds, targets)  # 예측값과 타겟 간의 손실 계산\n","        loss.backward()  # 손실에 대한 기울기 역전파\n","        optimizer.step()  # 모델의 가중치 업데이트\n","\n","        train_loss += loss.item()  # 손실 누적\n","        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())  # 예측값을 리스트에 추가\n","        targets_list.extend(targets.detach().cpu().numpy())  # 실제 값을 리스트에 추가\n","\n","        pbar.set_description(f\"Loss: {loss.item():.4f}\")  # 진행률 표시줄에 현재 손실 표시\n","\n","    train_loss /= len(loader)  # 평균 손실 계산\n","    train_acc = accuracy_score(targets_list, preds_list)  # 정확도 계산\n","    train_f1 = f1_score(targets_list, preds_list, average='macro')  # F1 점수 계산\n","\n","    ret = {\n","        \"train_loss\": train_loss,  # 평균 손실\n","        \"train_acc\": train_acc,  # 정확도\n","        \"train_f1\": train_f1,  # F1 점수\n","    }\n","\n","    return ret  # 결과 반환\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Wjom43UvoXcx"},"source":["## 3. Hyper-parameters\n","* 학습 및 추론에 필요한 하이퍼파라미터들을 정의합니다."]},{"cell_type":"markdown","metadata":{},"source":["#### 이미지 분류에 효율적인 모델들은 주로 CNN(Convolutional Neural Network)과 최근의 Transformer 기반 모델로 나눌 수 있습니다. 각 큰 분류별로 대표적인 모델과 그 특징을 설명하겠습니다.\n","\n","1. CNN 기반 모델\n","CNN 기반 모델은 주로 이미지의 공간적 정보를 효과적으로 처리하는데 강점을 가지며, 다양한 크기의 필터를 사용하여 이미지 특징을 추출합니다.\n","\n","- ResNet (Residual Networks)\n","모델명: resnet18, resnet34, resnet50, resnet101, resnet152\n","특징: 깊은 신경망에서 발생할 수 있는 기울기 소실 문제를 해결하기 위해 잔차 연결(residual connection)을 도입. 다양한 깊이의 모델이 존재하며, 각 모델은 이미지 분류에서 매우 높은 성능을 보여줌.\n","- DenseNet (Densely Connected Networks)\n","모델명: densenet121, densenet161, densenet169, densenet201\n","특징: 각 레이어가 이전 모든 레이어의 출력을 입력으로 받는 구조. 파라미터 효율성이 높고, 강력한 특징 추출 능력을 보임.\n","- EfficientNet\n","모델명: efficientnet_b0, efficientnet_b1, efficientnet_b2, efficientnet_b3, efficientnet_b4, efficientnet_b5, efficientnet_b6, efficientnet_b7\n","특징: 모델 크기와 컴퓨팅 자원을 효율적으로 사용하는 네트워크. 성능과 효율성 사이의 균형이 잘 맞음.\n","- MobileNet\n","모델명: mobilenetv2_035, mobilenetv2_050, mobilenetv2_075, mobilenetv2_100\n","특징: 모바일 및 임베디드 기기에서 효율적으로 동작하도록 설계. 경량화된 구조로 빠른 연산 속도를 자랑.\n","2. Transformer 기반 모델\n","최근의 Vision Transformer (ViT) 모델은 Transformer 구조를 이미지 분류에 적용한 것으로, 이미지 패치를 시퀀스로 변환하여 처리합니다.\n","\n","- Vision Transformer (ViT)\n","모델명: vit_base_patch16_224, vit_base_patch32_224, vit_large_patch16_224, vit_large_patch32_224\n","특징: Transformer 구조를 기반으로 이미지 패치를 처리. 높은 성능을 보이며, 특히 대규모 데이터셋에서 좋은 성능을 나타냄.\n","- DeiT (Data-efficient Image Transformers)\n","모델명: deit_base_patch16_224, deit_small_patch16_224, deit_tiny_patch16_224\n","특징: 데이터 효율성을 개선한 ViT 변형. 상대적으로 적은 데이터로도 높은 성능을 발휘.\n","- Swin Transformer (Shifted Window Transformer)\n","모델명: swin_base_patch4_window7_224, swin_large_patch4_window7_224\n","특징: 윈도우 기반의 Transformer로, 지역적 특징을 효과적으로 추출. 다양한 스케일에서 좋은 성능을 보임.\n","3. Hybrid 모델\n","Hybrid 모델은 CNN과 Transformer의 장점을 결합하여 효율성과 성능을 극대화한 모델입니다.\n","\n","- CoAtNet (Convolution and self-Attention Network)\n","모델명: coatnet_0_224, coatnet_1_224, coatnet_2_224\n","특징: Convolution과 self-Attention 메커니즘을 결합하여 이미지 분류 성능을 극대화. 효율적인 구조로 높은 성능을 보임.\n","- ConvNeXt\n","모델명: convnext_tiny, convnext_small, convnext_base, convnext_large\n","특징: ConvNeXt는 전통적인 CNN 구조를 현대적인 개선을 통해 Transformer와 경쟁할 수 있는 성능을 보임.\n","\n","추천\n","- 효율성과 성능을 동시에 고려할 때: EfficientNet 시리즈는 다양한 크기와 성능을 제공하므로, 모바일부터 고성능 서버까지 폭넓게 사용 가능.\n","- 최신 기술을 활용하고자 할 때: Vision Transformer (ViT)와 DeiT는 최신 Transformer 기술을 기반으로 높은 성능을 보장.\n","- Hybrid 모델의 성능을 원할 때: CoAtNet과 ConvNeXt는 CNN과 Transformer의 장점을 결합하여 최고의 성능을 제공.\n","이러한 모델 중에서 선택은 사용자의 요구사항과 환경에 따라 달라질 수 있습니다. 각 모델의 상세한 특성과 벤치마크 결과를 참고하여 최적의 모델을 선택하는 것이 중요합니다."]},{"cell_type":"markdown","metadata":{},"source":["#### 계속"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":436,"status":"ok","timestamp":1700315112439,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"KByfAeRmXwYk"},"outputs":[],"source":["# device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# data config\n","data_path = '/home/data/'\n","home_path = '/home/'\n","\n","# model config\n","model_name = 'efficientnet_b2' #'convnext_small' # 'resnet34' 'resnet50' 'efficientnet-b0', ...\n","\n","# training config\n","img_size = 32\n","#img_size = 224\n","LR = 1e-3\n","EPOCHS = 1\n","BATCH_SIZE = 32\n","num_workers = 0"]},{"cell_type":"markdown","metadata":{},"source":["이미지가 기울어져 있거나 뒤집혀 있는 경우, 이러한 변형을 처리할 수 있는 모델과 전처리 방법을 고려해야 합니다. 이미지 변형에 대해 강건한 모델과 전처리 방법을 추천드리겠습니다.\n","\n","1. 모델 선택\n","기울어지거나 뒤집힌 이미지를 처리하는 데 있어 강건한 모델로는 다음과 같은 모델들이 있습니다:\n","- ResNet (Residual Networks)\n","특징: 잔차 연결을 사용하여 깊은 신경망에서도 효과적으로 학습을 수행. 다양한 변형에 대해 비교적 강건함.\n","- EfficientNet\n","특징: 효율적이고 강력한 성능을 제공하며, 데이터 증강을 통해 다양한 이미지 변형에 대해 강건할 수 있음.\n","- Vision Transformer (ViT)\n","특징: 이미지 패치를 입력 받아 처리하므로, 각 패치의 상대적 위치에 덜 민감할 수 있음. 기울어짐 뒤집힘에 대한 강건성을 데이터 증강을 통해 강화 가능.\n","- Swin Transformer\n","특징: Shifted Window 방식을 사용하여 지역적 특징을 잘 포착. 다양한 변형에 대해 강건한 성능을 보일 수 있음."]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                         name\n","201        efficientformer_l1\n","202        efficientformer_l3\n","203        efficientformer_l7\n","204       efficientformerv2_l\n","205      efficientformerv2_s0\n","206      efficientformerv2_s1\n","207      efficientformerv2_s2\n","208           efficientnet_b0\n","209     efficientnet_b0_g8_gn\n","210  efficientnet_b0_g16_evos\n","211        efficientnet_b0_gn\n","212           efficientnet_b1\n","213    efficientnet_b1_pruned\n","214           efficientnet_b2\n","215    efficientnet_b2_pruned\n","216           efficientnet_b3\n","217     efficientnet_b3_g8_gn\n","218        efficientnet_b3_gn\n","219    efficientnet_b3_pruned\n","220           efficientnet_b4\n","221           efficientnet_b5\n","222           efficientnet_b6\n","223           efficientnet_b7\n","224           efficientnet_b8\n","225     efficientnet_cc_b0_4e\n","226     efficientnet_cc_b0_8e\n","227     efficientnet_cc_b1_8e\n","228           efficientnet_el\n","229    efficientnet_el_pruned\n","230           efficientnet_em\n","231           efficientnet_es\n","232    efficientnet_es_pruned\n","233           efficientnet_l2\n","234        efficientnet_lite0\n","235        efficientnet_lite1\n","236        efficientnet_lite2\n","237        efficientnet_lite3\n","238        efficientnet_lite4\n","239          efficientnetv2_l\n","240          efficientnetv2_m\n","241       efficientnetv2_rw_m\n","242       efficientnetv2_rw_s\n","243       efficientnetv2_rw_t\n","244          efficientnetv2_s\n","245         efficientnetv2_xl\n","246           efficientvit_b0\n","247           efficientvit_b1\n","248           efficientvit_b2\n","249           efficientvit_b3\n","250           efficientvit_l1\n","251           efficientvit_l2\n","252           efficientvit_l3\n","253           efficientvit_m0\n","254           efficientvit_m1\n","255           efficientvit_m2\n","256           efficientvit_m3\n","257           efficientvit_m4\n","258           efficientvit_m5\n"]}],"source":["bat = 'effi'\n","\n","### 모델 리스트\n","all_densenet_models = timm.list_models('*')\n","\n","aldf = pd.DataFrame(all_densenet_models)\n","aldf.columns = ['name']\n","\n","# 'name' 컬럼에서 'bat'로 시작하는 행 필터링\n","filtered_df = aldf[aldf['name'].str.startswith(bat)]\n","print(filtered_df)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/data/data.csv\n"]}],"source":["print(data_path + \"data.csv\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"amum-FlIojc6"},"source":["## 4. Load Data\n","* 학습, 테스트 데이터셋과 로더를 정의합니다."]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1700315112439,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"llh5C7ZKoq2S"},"outputs":[],"source":["\"\"\"\n","# augmentation을 위한 transform 코드\n","trn_transform = A.Compose([\n","    # 이미지 크기 조정\n","    A.Resize(height=img_size, width=img_size),\n","    # images normalization\n","    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    # numpy 이미지나 PIL 이미지를 PyTorch 텐서로 변환\n","    ToTensorV2(),\n","])\n","\n","# test image 변환을 위한 transform 코드\n","tst_transform = A.Compose([\n","    A.Resize(height=img_size, width=img_size),\n","    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ToTensorV2(),\n","])\n","\"\"\"\n","\n","# GPT 추천 \n","# 이미지 변환을 위한 Albumentations 파이프라인 정의\n","trn_transform = A.Compose([\n","    # 45도 범위 내에서 랜덤 회전\n","    A.Rotate(limit=90, p=1.0),\n","    # 좌우 뒤집기\n","    A.HorizontalFlip(p=0.5),\n","    # 상하 뒤집기\n","    A.VerticalFlip(p=0.5),\n","    # 아핀 변환 (translation)\n","    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=1.0),\n","    # 이미지 크기 조정\n","    A.Resize(height=img_size, width=img_size),\n","    # 이미지 정규화\n","    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    # 이미지를 PyTorch 텐서로 변환\n","    ToTensorV2(),\n","])\n","\n","# test image 변환을 위한 transform 코드\n","tst_transform = A.Compose([\n","    A.Resize(height=img_size, width=img_size),\n","    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ToTensorV2(),\n","])"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1700315112808,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"INxdmsStop2L","outputId":"49f0d412-8ce6-4d2f-ae78-d5cf3d056340"},"outputs":[{"name":"stdout","output_type":"stream","text":["1570 3140\n"]}],"source":["# Dataset 정의\n","trn_dataset = ImageDataset(\n","    data_path + \"train.csv\",\n","    data_path + \"train/\",\n","    transform=trn_transform\n",")\n","tst_dataset = ImageDataset(\n","    data_path + \"sample_submission.csv\",\n","    data_path + \"test/\",\n","    transform=tst_transform\n",")\n","print(len(trn_dataset), len(tst_dataset))"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1700315112808,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"_sO03fWaQj1h"},"outputs":[],"source":["# DataLoader 정의\n","trn_loader = DataLoader(\n","    trn_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=num_workers,\n","    pin_memory=True,\n","    drop_last=False\n",")\n","tst_loader = DataLoader(\n","    tst_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0,\n","    pin_memory=True\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Nmm5h3J-pXNV"},"source":["## 5. Train Model\n","* 모델을 로드하고, 학습을 진행합니다."]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":870,"status":"ok","timestamp":1700315114067,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"FbBgFPsLT-CO"},"outputs":[],"source":["# load model\n","model = timm.create_model(\n","    model_name,\n","    pretrained=False,  # RuntimeError: No pretrained weights exist for coatnet_0_224. Use `pretrained=False` for random init.\n","    num_classes=17\n",").to(device)\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=LR)"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8778,"status":"ok","timestamp":1700315122843,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"OvIVcSRgUPtS","outputId":"88230bf2-976f-45f6-b3b7-1a2d0ad00548"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loss: 6.6409: 100%|██████████| 50/50 [00:11<00:00,  4.28it/s]"]},{"name":"stdout","output_type":"stream","text":["train_loss: 5.2883\n","train_acc: 0.1102\n","train_f1: 0.1017\n","epoch: 0.0000\n","\n","CPU times: user 29.4 s, sys: 8.74 s, total: 38.1 s\n","Wall time: 11.7 s\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["%%time \n","#baseline 14.8s | \n","#Loss: 2.5325: 100%|██████████| 50/50 [00:06<00:00,  7.65it/s]\n","#train_loss: 2.4731\n","#train_acc: 0.2554\n","#train_f1: 0.2238\n","#epoch: 0.0000\n","\n","# 이게 기본값인가?\n","#Loss: 1.8296: 100%|██████████| 50/50 [00:09<00:00,  5.28it/s]\n","#train_loss: 1.6996\n","#train_acc: 0.5682\n","#train_f1: 0.5234\n","#epoch: 0.0000\n","#CPU times: user 22.1 s, sys: 624 ms, total: 22.7 s\n","#Wall time: 9.47 s\n","\n","for epoch in range(EPOCHS):\n","    ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device=device)\n","    ret['epoch'] = epoch\n","\n","    log = \"\"\n","    for k, v in ret.items():\n","      log += f\"{k}: {v:.4f}\\n\"\n","    print(log)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loss: 5.9813: 100%|██████████| 50/50 [00:10<00:00,  4.82it/s]"]},{"name":"stdout","output_type":"stream","text":["train_loss: 2.8855\n","train_acc: 0.1522\n","train_f1: 0.1284\n","epoch: 0.0000\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["for epoch in range(EPOCHS):\n","    ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device=device)\n","    ret['epoch'] = epoch\n","\n","    log = \"\"\n","    for k, v in ret.items():\n","      log += f\"{k}: {v:.4f}\\n\"\n","    print(log)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lkwxRXoBpbaX"},"source":["# 6. Inference & Save File\n","* 테스트 이미지에 대한 추론을 진행하고, 결과 파일을 저장합니다."]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12776,"status":"ok","timestamp":1700315185336,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"uRYe6jlPU_Om","outputId":"2a08690c-9ffe-418d-8679-eb9280147110"},"outputs":[],"source":["preds_list = []\n","\n","model.eval()  # 모델을 평가 모드로 전환\n","for image, _ in tst_loader:  #tqdm(tst_loader): # 테스트 데이터 로더에서 이미지 가져오기\n","    image = image.to(device)  # 이미지를 GPU로 전송\n","\n","    with torch.no_grad():  # 기울기 계산 비활성화\n","        preds = model(image)  # 예측 수행\n","    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())  # 예측 결과 처리 및 저장"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1700315216829,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"aClN7Qi7VZoh"},"outputs":[],"source":["### test 데이터셋 분류 번호 붙이기\n","pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])\n","pred_df['target'] = preds_list"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1700315238836,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"VDBXQqAzVvLY"},"outputs":[],"source":["sample_submission_df = pd.read_csv(data_path + \"sample_submission.csv\")\n","assert (sample_submission_df['ID'] == pred_df['ID']).all()"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["현재 한국 시간: 0730_1720\n"]}],"source":["from datetime import datetime\n","import pytz\n","\n","# 현재 UTC 시간\n","utc_now = datetime.utcnow()\n","\n","# 한국 시간대\n","kst = pytz.timezone('Asia/Seoul')\n","\n","# 한국 시간으로 변환\n","kst_now = utc_now.astimezone(kst)\n","\n","# 원하는 포맷으로 시간 출력\n","current_time_kst = kst_now.strftime(\"%m%d_%H%M\")\n","print(\"현재 한국 시간:\", current_time_kst)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["coding = input(\"변경사항: \")"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":317,"status":"ok","timestamp":1700315244710,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"ePx2vCELVnuS"},"outputs":[],"source":["if coding != \"\" :\n","    pred_df.to_csv(home_path + \"pred\" + current_time_kst + \"_\" + coding + \".csv\", index=False)\n","else:\n","    print(\"pred 파일로 저장하지 않음.\")"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":353,"status":"ok","timestamp":1700315247734,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"9yMO8s6GqAwZ","outputId":"9a30616f-f0ea-439f-a906-dd806737ce00"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0008fdb22ddce0ce.jpg</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00091bffdffd83de.jpg</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00396fbc1f6cc21d.jpg</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00471f8038d9c4b6.jpg</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00901f504008d884.jpg</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     ID  target\n","0  0008fdb22ddce0ce.jpg       2\n","1  00091bffdffd83de.jpg       3\n","2  00396fbc1f6cc21d.jpg       5\n","3  00471f8038d9c4b6.jpg       3\n","4  00901f504008d884.jpg       2"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["pred_df.head()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
